import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.cluster import DBSCAN
from sklearn.metrics import classification_report, roc_auc_score, roc_curve, ConfusionMatrixDisplay

# ======================================
# 1) Carregar dataset
# ======================================
df = pd.read_excel("/home/vitor/projetos-amf/inteligencia-artificial/trabalho_ia/reports/Herd Data 18-08-2024.xlsx")

# Visualiza√ß√£o r√°pida das colunas dispon√≠veis no dataset:
# ['Cow No.', 'Cow Name', 'Group No.', 'Previous Group No.', 'Group Change Date',
#  'Days Since Group Change', 'Sex', 'Days In Pregnancy', 'Insem Date', 'Breed', 
#  'No. of Lact.', 'Days In Milk', 'Cull flagged', 'Birth Date', 'Age Years', 
#  'Days Since Calving', 'Tot. Milk Yest.', 'Milk This Month', ... etc]

# ======================================
# 2) Pr√©-processamento e Engenharia de Atributos
# ======================================

# Converter data de nascimento para datetime
df["Birth Date"] = pd.to_datetime(df["Birth Date"], errors="coerce")

# Convers√£o de colunas num√©ricas
numeric_cols_try = [
    "Tot. Milk Yest.", "Yield This Week ", "Milk Last Month",
    "Milk This Month", "Days In Milk", "Days Since Calving",
    "Calving Interval", "Age Years"
]
for c in numeric_cols_try:
    if c in df.columns:
        df[c] = pd.to_numeric(df[c], errors="coerce")

# Idade calculada a partir da data de nascimento
if "Birth Date" in df.columns:
    df["Age Years_calc"] = (pd.to_datetime("today") - df["Birth Date"]).dt.days // 365
    df["Age Years"] = df["Age Years"].fillna(df["Age Years_calc"])
    df.drop(columns=["Age Years_calc"], inplace=True)

# M√©dia da produ√ß√£o mensal de leite (se dispon√≠vel)
monthly_cols = [c for c in df.columns if c.startswith("Monthly Milk")]
if monthly_cols:
    df["Monthly Milk Avg"] = df[monthly_cols].apply(pd.to_numeric, errors="coerce").mean(axis=1)

# Queda de produ√ß√£o em rela√ß√£o √† m√©dia mensal
if "Monthly Milk Avg" in df.columns and "Tot. Milk Yest." in df.columns:
    df["Milk Drop"] = df["Monthly Milk Avg"] - df["Tot. Milk Yest."]

# Risco p√≥s-parto baseado nos dias desde o parto
if "Days Since Calving" in df.columns:
    df["Days Since Calving"] = pd.to_numeric(df["Days Since Calving"], errors="coerce")
    df["Postpartum Risk"] = pd.cut(
        df["Days Since Calving"],
        bins=[-1, 30, 90, 365, float("inf")],
        labels=["alto", "medio", "baixo", "muito_baixo"],
        ordered=True
    )

# Categorias de produ√ß√£o de leite (explora√ß√£o apenas, n√£o usada no modelo)
if "Tot. Milk Yest." in df.columns:
    milk_bins = [0, 10, 20, float("inf")]
    milk_labels = ["baixa", "media", "alta"]
    df["Milk Category"] = pd.cut(df["Tot. Milk Yest."], bins=milk_bins, labels=milk_labels, ordered=True)

# ======================================
# 3) Defini√ß√£o da vari√°vel alvo
# ======================================
# R√≥tulo bin√°rio: 1 = risco de baixa produ√ß√£o (<10 litros/dia), 0 = produ√ß√£o normal
if "Tot. Milk Yest." not in df.columns:
    raise ValueError("Coluna 'Tot. Milk Yest.' n√£o encontrada no dataset.")
df["target_low_milk"] = (df["Tot. Milk Yest."] < 10).astype(int)

# ======================================
# 4) Sele√ß√£o de atributos (evitar vazamento de dados)
# ======================================
leak_cols = {"Tot. Milk Yest.", "Milk Category", "Milk Drop"}  # colunas que n√£o devem ser usadas
candidate_features = [
    "Age Years", "No. of Lact.", "Days In Milk", "Days Since Calving", "Calving Interval",
    "Yield This Week ", "Milk Last Month", "Milk This Month", "Monthly Milk Avg",
    "Pregnant", "Breeding Status", "Breeding State", "Breed", "Sex", "Delivery", "Postpartum Risk"
]
features = [c for c in candidate_features if c in df.columns and c not in leak_cols]

# Base final para ML
df_ml = df[features + ["target_low_milk"]].dropna(subset=["target_low_milk"]).copy()

# Separa√ß√£o entre atributos num√©ricos e categ√≥ricos
numeric_features = [c for c in features if pd.api.types.is_numeric_dtype(df_ml[c])]
categorical_features = [c for c in features if not pd.api.types.is_numeric_dtype(df_ml[c])]

print("Atributos num√©ricos:", numeric_features)
print("Atributos categ√≥ricos:", categorical_features)

# ======================================
# 5) Pipelines de pr√©-processamento
# ======================================
numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])

preprocessor = ColumnTransformer(transformers=[
    ("num", numeric_transformer, numeric_features),
    ("cat", categorical_transformer, categorical_features)
])

# ======================================
# 6) Divis√£o treino/teste
# ======================================
X = df_ml.drop(columns=["target_low_milk"])
y = df_ml["target_low_milk"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.20, random_state=42, stratify=y
)

# ======================================
# 7) Modelo supervisionado: RandomForest
# ======================================
rf_model = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("classifier", RandomForestClassifier(
        n_estimators=300,
        random_state=42,
        class_weight="balanced"
    ))
])

# Valida√ß√£o cruzada
scores = cross_val_score(rf_model, X, y, cv=5, scoring="roc_auc")
print(f"\nüîé AUC m√©dio (5-fold CV): {scores.mean():.3f} ¬± {scores.std():.3f}")

# Treinamento no conjunto de treino
rf_model.fit(X_train, y_train)

# Predi√ß√µes
y_pred = rf_model.predict(X_test)
y_prob = rf_model.predict_proba(X_test)[:, 1]

print("\nüìä Relat√≥rio de Classifica√ß√£o (RandomForest):")
print(classification_report(y_test, y_pred, target_names=["Produ√ß√£o Normal", "Risco Baixa Produ√ß√£o"]))

# Matriz de Confus√£o
ConfusionMatrixDisplay.from_predictions(
    y_test, y_pred,
    display_labels=["Produ√ß√£o Normal", "Risco Baixa Produ√ß√£o"],
    cmap="Blues"
)
plt.title("Matriz de Confus√£o - RandomForest", fontsize=14, pad=20)
plt.show()

# Curva ROC
fpr, tpr, _ = roc_curve(y_test, y_prob)
plt.plot(fpr, tpr, label=f"AUC = {roc_auc_score(y_test, y_prob):.2f}")
plt.plot([0, 1], [0, 1], "--", color="gray")
plt.xlabel("Taxa de Falsos Positivos")
plt.ylabel("Taxa de Verdadeiros Positivos")
plt.title("Curva ROC - RandomForest")
plt.legend()
plt.show()

# ======================================
# 8) Import√¢ncia das vari√°veis
# ======================================
rf = rf_model.named_steps["classifier"]
ohe = rf_model.named_steps["preprocessor"].named_transformers_["cat"].named_steps["onehot"]
cat_names = ohe.get_feature_names_out(categorical_features)
all_features = numeric_features + list(cat_names)

feat_imp = pd.Series(rf.feature_importances_, index=all_features).sort_values(ascending=False).head(15)

plt.figure(figsize=(10,6))
sns.barplot(x=feat_imp.values, y=feat_imp.index)
plt.title("Import√¢ncia das Vari√°veis - RandomForest")
plt.xlabel("Import√¢ncia Relativa")
plt.ylabel("Vari√°veis")
plt.show()

# ======================================
# 9) Modelo n√£o supervisionado: DBSCAN (Detec√ß√£o de anomalias)
# ======================================
X_scaled = preprocessor.fit_transform(X)

dbscan = DBSCAN(eps=1.5, min_samples=5)
clusters = dbscan.fit_predict(X_scaled)

df_ml["cluster_dbscan"] = clusters
df_ml["cluster_dbscan_label"] = df_ml["cluster_dbscan"].apply(lambda x: "Outlier" if x == -1 else f"Cluster {x}")

# Contagem dos clusters
cluster_counts = df_ml["cluster_dbscan"].value_counts().sort_index()
total = cluster_counts.sum()

print("\nüìä Contagem de clusters DBSCAN:")
cluster_summary = cluster_counts.to_frame(name="Count")
cluster_summary["Percent"] = (cluster_summary["Count"] / total * 100).round(1)
print(cluster_summary)

# Gr√°fico de distribui√ß√£o de clusters
plt.figure(figsize=(8,5))
bars = plt.bar(cluster_counts.index, cluster_counts.values, color="skyblue", edgecolor="black")
for bar, idx in zip(bars, cluster_counts.index):
    if idx == -1:
        bar.set_color("salmon")  # Outliers em vermelho
    pct = (cluster_counts[idx] / total) * 100
    plt.text(idx, cluster_counts[idx] + 0.5, f"{cluster_counts[idx]} ({pct:.1f}%)", ha="center")
plt.title("Distribui√ß√£o dos Clusters DBSCAN (-1 = Outliers)", fontsize=14, pad=15)
plt.xlabel("Cluster")
plt.ylabel("N√∫mero de vacas")
plt.show()

# Vacas an√¥malas
anomalias = df_ml[df_ml["cluster_dbscan"] == -1]
print(f"\nüö® Vacas an√¥malas detectadas (cluster -1): {len(anomalias)}")
print(anomalias.head())

# Visualiza√ß√£o em 2D com PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

df_plot = pd.DataFrame(X_pca, columns=["PC1", "PC2"])
df_plot["Cluster"] = df_ml["cluster_dbscan_label"]

plt.figure(figsize=(8,6))
palette = sns.color_palette("tab10", n_colors=len(cluster_counts))
palette_dict = {f"Cluster {i}": palette[i] for i in range(len(cluster_counts))}
palette_dict["Outlier"] = "red"

sns.scatterplot(
    data=df_plot, x="PC1", y="PC2",
    hue="Cluster", palette=palette_dict,
    alpha=0.7
)
plt.title("Clusters DBSCAN (redu√ß√£o PCA) - Outliers em vermelho", fontsize=14)
plt.show()
